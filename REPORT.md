# Отчёт по прототипу аналитической системы

## 1. Краткое описание задачи и подхода

CLI-прототип анализирует статистику по фрилансерам (доходы, регионы, опыт, платформы и др.) и отвечает на вопросы на естественном языке через LLM. LLM не видит сырые данные, а только агрегаты через tools. Все вычисления — на стороне DataAnalyzer (Python).

## 2. Архитектура и стек

- Python 3.11+
- DataAnalyzer: кастомный класс, агрегирует CSV через стандартный csv
- LangChain (agents, tools, интеграция с LLM)
- LLM: GigaChat, Groq (Llama-3, Mixtral, Gemma) — на выбор
- CLI-интерфейс, поддержка диалога
- Конфигурирование через pydantic BaseSettings
- Валидация входных данных и параметров инструментов через pydantic

### 2.1. Особенности типизации инструментов и передачи списка методов для LLM

- Для GigaChat требуется строгая типизация аргументов инструментов (tools), через Pydantic-схемы с явным описанием всех полей. В Groq и OpenAI строгая типизация тоже работает, но иногда можно обойтись и более размытыми схемами — однако для кросс-LLM-совместимости используем строгую типизацию.
- Если не передавать LLM список доступных методов (METHODS_LIST), GigaChat не сможет корректно выбрать и вызвать нужный инструмент, даже если tool зарегистрирован в агенте. Groq и OpenAI находят методы даже с минимальным описанием в @tool, но это нестабильно.
- Поэтому рекомендуется всегда включать METHODS_LIST в system prompt или context для LLM. Это увеличивает размер prompt, но гарантирует, что LLM будет знать, какие инструменты доступны.
- Для Groq (и других OpenAI-совместимых LLM) введено ограничение: в batch_analytics за один запрос обрабатывается не более N метрик (settings.max_batch_methods, по умолчанию 15). Если методов больше — обрабатываются только первые N. Это сделано потому, что Groq-LLM часто пытается вызвать все возможные методы разом, что приводит к переполнению контекста и ошибкам API. В GigaChat такой проблемы нет.
- Также в system prompt добавлена инструкция: не смешивать методы с параметром by и без by в одном batch-запросе, для них делаются отдельные вызовы.

## 3. Оценка эффективности и точности

- Время ответа на аналитический запрос: ~1 мс (DataAnalyzer), 2–10 сек (LLM)
- Точность аналитики = точность функций DataAnalyzer (LLM не "галлюцинирует" данные)
- CLI-интерфейс интуитивен, поддерживает диалог и сводные запросы
- Логирование времени выполнения и ошибок
- Не требует тяжёлых библиотек (pandas не используется)

## 4. Методы, технологии, что сработало/нет

**Сработало:**

- Разделение аналитики и LLM-интерфейса
- Универсальный batch_analytics для сводных отчётов
- Быстрая обработка CSV без pandas
- Лёгкое добавление новых аналитических функций

**Не сработало/нюансы:**

- LLM иногда некорректно сериализует аргументы для batch_analytics (требуется pre-processing)
- LLM может "отказываться" вызывать tool после ошибки (требуется post-processing)
- Медленный отклик из-за latency LLM (особенно при длинных промптах)

## 5. Критерии самооценки

- Корректность и полнота аналитических функций (покрытие всех вопросов из ТЗ и дополнительных)
- Время отклика (CLI <1 сек, LLM <10 сек)
- Устойчивость к ошибкам (валидация входных данных, обработка edge-cases)
- Расширяемость (добавление новых функций без переписывания архитектуры)
- Удобство CLI (help, примеры)

## 6. Рекомендации по развитию

- Реализовать динамику по времени (если есть даты)
- Добавить квантильные/медианные разрезы (не только средние)
- Визуализации (ASCII-графики, sparkline)
- Поддержка выгрузки отчётов (txt/csv)
- Интеграция с другими LLM (Anthropic, Gemini)

## 7. Основные аналитические функции (с примерами)

- **Сравнение дохода по крипте/фиату:**
  - Насколько выше доход у фрилансеров, принимающих оплату в криптовалюте?
- **Доход по регионам:**
  - Как распределяется доход фрилансеров по регионам?
- **Процент экспертов с <100 проектов:**
  - Какой процент экспертов выполнил менее 100 проектов?
- **Средний доход по категориям/опыту/платформам/типу проекта**
- **Топ-5 регионов по экспертам**
- **Процент с повторным наймом >50%**
- **Среднее время выполнения (по всем, по категориям, регионам, опыту, платформам, типу проекта)**
- **Средняя ставка (Hourly Rate) по категориям/регионам/опыту/платформам/типу проекта**
- **Средний Job Success Rate**
- **Средний рейтинг клиента**
- **Средние маркетинговые расходы**
- **Универсальный batch_analytics для сводных отчётов**

**Примеры запросов:**

- "Покажи топ-5 регионов по экспертам"
- "Сравни доход по крипте и обычным способом"
- "Среднее время выполнения по категориям и регионам"
- "Сделай сводку по всем доступным метрикам"

## 8. Тестирование и покрытие

- Для всех аналитических функций реализованы unit-тесты (pytest), покрывающие:
- Корректную работу на валидных данных (доходы, регионы, опыт, платформы, категории и т.д.)
- Обработку пустых, битых, частичных данных (edge-cases, отсутствие ключей, нулевые/отрицательные значения)
- Тесты разделены на основные (валидные сценарии) и edge (устойчивость к ошибкам и невалидным данным)
- Все обращения к данным защищены через .get(...), KeyError невозможен
- Покрытие: 100% всех публичных аналитических методов DataAnalyzer
- Все тесты проходят (pytest green), система устойчива к ошибкам данных

## 9. Docker и контейнеризация

- Проект полностью контейнеризован для удобства запуска и тестирования.
- Используется Poetry для управления зависимостями (pyproject.toml).
- В репозитории есть Dockerfile и .dockerignore.
- Для сложных сценариев (volume, переменные, несколько сервисов) — docker-compose.yml.

**Инструкция по запуску:**

```bash
# Собрать образ
docker build -t freelance-analytics-cli .
# Запустить контейнер
# (CSV-файлы должны лежать в ./data, пробрасываются внутрь контейнера)
docker run --rm -it -e GROQ_API_KEY=sk-...твой_ключ... -v $(pwd)/data:/app/data freelance-analytics-cli

# Или через docker-compose
docker-compose run --rm freelance-analytics-cli

# Отдельно запуск тестов
docker-compose run --rm test
```

**Плюсы:**

- Гарантированная воспроизводимость окружения
- Быстрый старт без ручной установки зависимостей
- Легко деплоить на любой сервер/облако
- Можно расширять под несколько сервисов (LLM, БД и т.д.)

---
