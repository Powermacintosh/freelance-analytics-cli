# Отчёт по прототипу аналитической системы

## 1. Краткое описание задачи и подхода

CLI-прототип анализирует статистику по фрилансерам (доходы, регионы, опыт, платформы и др.) и отвечает на вопросы на естественном языке через LLM. LLM не видит сырые данные, а только агрегаты через tools. Все вычисления — на стороне DataAnalyzer (Python).

## 2. Архитектура и стек

- Python 3.11+
- DataAnalyzer: кастомный класс, агрегирует CSV через стандартный csv
- LangChain (agents, tools, интеграция с LLM)
- LLM: GigaChat, Groq (Llama-3, Mixtral, Gemma) — на выбор
- CLI-интерфейс, поддержка диалога
- Конфигурирование через pydantic BaseSettings
- Валидация входных данных и параметров инструментов через pydantic

### 2.1. Особенности типизации инструментов и передачи списка методов для LLM

- Для GigaChat требуется строгая типизация аргументов инструментов (tools), через Pydantic-схемы с явным описанием всех полей. В Groq и OpenAI строгая типизация тоже работает, но иногда можно обойтись и более размытыми схемами — однако для кросс-LLM-совместимости используем строгую типизацию.
- Если не передавать LLM список доступных методов (METHODS_LIST), GigaChat не сможет корректно выбрать и вызвать нужный инструмент, даже если tool зарегистрирован в агенте. Groq и OpenAI находят методы даже с минимальным описанием в @tool.
- Поэтому для GiGaChat рекомендуется всегда включать METHODS_LIST в system prompt или context для LLM. Это увеличивает размер prompt, но гарантирует, что LLM будет знать, какие инструменты доступны.
- Для Groq (и других OpenAI-совместимых LLM) введено ограничение: в batch_analytics за один запрос обрабатывается не более N метрик (settings.max_batch_methods, по умолчанию 15). Если методов больше — обрабатываются только первые N. Это сделано потому, что Groq-LLM часто пытается вызвать все возможные методы разом, что приводит к переполнению контекста и ошибкам API. В GigaChat такой проблемы нет, потому что там значительно больше лимит.

### 2.2. Стратегия хранения сообщений

Теперь в state хранятся два списка сообщений:

1. `messages` - полная история диалога, включая все сообщения пользователя и ответы
2. `llm_input_messages` - обрезанная версия для модели, содержащая только последние N пар

Это позволяет модели видеть только релевантный контекст, не перегружая его всей историей. При этом полная история сохраняется в `messages` для возможного использования в будущем.

## 3. Оценка эффективности и точности

- Время ответа на аналитический запрос: ~1 мс (DataAnalyzer), 2–10 сек (LLM)
- Точность аналитики = точность функций DataAnalyzer (LLM не "галлюцинирует" данные)
- CLI-интерфейс интуитивен, поддерживает диалог и сводные запросы
- Логирование времени выполнения и ошибок
- Не требует тяжёлых библиотек (pandas не используется)

## 4. Методы, технологии, что сработало/нет

**Сработало:**

- Разделение аналитики и LLM-интерфейса
- Универсальный batch_analytics для сводных отчётов
- Быстрая обработка CSV без pandas
- Лёгкое добавление новых аналитических функций

**Не сработало/нюансы:**

- LLM иногда некорректно сериализует аргументы для batch_analytics (требуется pre-processing)
- LLM может "отказываться" вызывать tool после ошибки (требуется post-processing)
- Медленный отклик из-за latency LLM (особенно при длинных промптах)

## 5. Критерии самооценки

- Корректность и полнота аналитических функций (покрытие всех вопросов из ТЗ и дополнительных)
- Время отклика (CLI <1 сек, LLM <10 сек)
- Устойчивость к ошибкам (валидация входных данных, обработка edge-cases)
- Расширяемость (добавление новых функций без переписывания архитектуры)
- Удобство CLI (help, примеры)

## 6. Рекомендации по развитию

- Реализовать динамику по времени (если есть даты)
- Добавить квантильные/медианные разрезы (не только средние)
- Визуализации (ASCII-графики, sparkline)
- Поддержка выгрузки отчётов (txt/csv)
- Интеграция с другими LLM (Anthropic, Gemini)

## 7. Основные аналитические функции (с примерами)

- **Сравнение дохода по крипте/фиату:**
  - Насколько выше доход у фрилансеров, принимающих оплату в криптовалюте?
- **Доход по регионам:**
  - Как распределяется доход фрилансеров по регионам?
- **Процент экспертов с <100 проектов:**
  - Какой процент экспертов выполнил менее 100 проектов?
- **Средний доход по категориям/опыту/платформам/типу проекта**
- **Топ-5 регионов по экспертам**
- **Процент с повторным наймом >50%**
- **Среднее время выполнения (по всем, по категориям, регионам, опыту, платформам, типу проекта)**
- **Средняя ставка (Hourly Rate) по категориям/регионам/опыту/платформам/типу проекта**
- **Средний Job Success Rate**
- **Средний рейтинг клиента**
- **Средние маркетинговые расходы**
- **Универсальный batch_analytics для сводных отчётов**

**Примеры запросов:**

- "Покажи топ-5 регионов по экспертам"
- "Сравни доход по крипте и обычным способом"
- "Среднее время выполнения по категориям и регионам"
- "Сделай сводку по всем доступным метрикам"

## 8. Тестирование и покрытие

- Для всех аналитических функций реализованы unit-тесты (pytest), покрывающие:
- Корректную работу на валидных данных (доходы, регионы, опыт, платформы, категории и т.д.)
- Обработку пустых, битых, частичных данных (edge-cases, отсутствие ключей, нулевые/отрицательные значения)
- Тесты разделены на основные (валидные сценарии) и edge (устойчивость к ошибкам и невалидным данным)
- Все обращения к данным защищены через .get(...), KeyError невозможен
- Покрытие: 100% всех публичных аналитических методов DataAnalyzer
- Все тесты проходят (pytest green), система устойчива к ошибкам данных

## 9. Docker и контейнеризация

- Проект полностью контейнеризован для удобства запуска и тестирования.
- Используется Poetry для управления зависимостями (pyproject.toml).
- В репозитории есть Dockerfile и .dockerignore.
- Для сложных сценариев (volume, переменные, несколько сервисов) — docker-compose.yml.

**Инструкция по запуску:**

```bash
# Собрать образ
docker build -t freelance-analytics-cli .
# Запустить контейнер
# (CSV-файлы должны лежать в ./data, пробрасываются внутрь контейнера)
docker run --rm -it -e GROQ_API_KEY=sk-...твой_ключ... -v $(pwd)/data:/app/data freelance-analytics-cli

# Или через docker-compose
docker-compose run --rm freelance-analytics-cli

# Отдельно запуск тестов
docker-compose run --rm test
```

**Плюсы:**

- Гарантированная воспроизводимость окружения
- Быстрый старт без ручной установки зависимостей
- Легко деплоить на любой сервер/облако
- Можно расширять под несколько сервисов (LLM, БД и т.д.)

## 10. LangGraph - управление сообщениями

- `messages` создает и управляет LangGraph. Это часть state, который LangGraph автоматически обновляет после каждого вызова.
- `llm_input_messages` создаем мы в `_pre_model_hook`, обрезая `messages` до нужного размера.
- `llm_input_messages` - это зарезервированное имя в LangGraph. Когда мы возвращаем из `_pre_model_hook` словарь с этим ключом, LangGraph знает, что нужно использовать эти сообщения для контекста LLM вместо `messages`.
- Если `llm_input_messages` не указан, LangGraph использует `messages` по умолчанию.

#### Детали хранения пар

- Количество пар ограничено `settings.max_history_pairs` (по умолчанию 6)
- Пары могут быть двух типов:
  1. Обычные пары: `HumanMessage` + `AIMessage` (ответ модели)
  2. Инструментальные пары: `AIMessage` (вызов инструмента) + `ToolMessage` (результат)
- Размер пар может отличаться:
  - Обычные пары: 2 сообщения
  - Инструментальные пары: 2-3 сообщения (если есть дополнительные метаданные)
- Системное сообщение (`SystemMessage`) всегда сохраняется первым и является отдельным сообщением (не входит в пары)
- Последний запрос пользователя всегда сохраняется последним
- При превышении лимита пар, старые пары удаляются, сохраняя только последние N
- **Важно**: при обрезке пары сохраняются в неизменном виде - если в исходной истории пара содержала 3 сообщения, то и в обрезанной версии она будет содержать те же 3 сообщения

#### Примеры пар и обрезки

```python
# Исходная история (10 пар)
messages = [
    # Блок 1: Системное сообщение
    # Всегда первое и единственное, не входит в пары
    SystemMessage("Ты — ассистент..."),

    # Блок 2: Пара 1 (Обычная)
    # Простой диалог: вопрос пользователя + ответ модели
    HumanMessage("Какой доход в крипте?"),
    AIMessage("Давайте посмотрим..."),

    # Блок 3: Пара 2 (Инструментальная)
    # Полный цикл: запрос -> вызов инструмента -> результат
    HumanMessage("Покажи статистику"),
    AIMessage(crypto_vs_other_income),
    ToolMessage("Средний доход: 5139.30 USD"),

    # Блок 4: Пара 3 (Обычная)
    # Снова простой диалог
    HumanMessage("А по регионам?"),
    AIMessage("Сейчас проверю..."),

    # Блок 5: Пара 4 (Инструментальная)
    # Полный цикл с другим инструментом
    HumanMessage("Покажи по регионам"),
    AIMessage(income_by_region),
    ToolMessage("Canada: 5350.13 USD..."),

    # Блок 6: Пара 5 (Обычная)
    # Промежуточный диалог
    HumanMessage("А эксперты?"),
    AIMessage("Посмотрю статистику..."),

    # Блок 7: Пара 6 (Инструментальная)
    # Запрос статистики по экспертам
    HumanMessage("Сколько экспертов?"),
    AIMessage(percent_experts_lt_100_projects),
    ToolMessage("33.9% экспертов..."),

    # Блок 8: Пара 7 (Обычная)
    # Уточняющий вопрос
    HumanMessage("А по категориям?"),
    AIMessage("Сейчас проверю..."),

    # Блок 9: Пара 8 (Инструментальная)
    # Запрос статистики по категориям
    HumanMessage("Покажи по категориям"),
    AIMessage(avg_income_by_category),
    ToolMessage("Design: 5200 USD..."),

    # Блок 10: Пара 9 (Обычная)
    # Еще один уточняющий вопрос
    HumanMessage("А по опыту?"),
    AIMessage("Посмотрю..."),

    # Блок 11: Пара 10 (Инструментальная)
    # Последний запрос статистики
    HumanMessage("Покажи по опыту"),
    AIMessage(avg_income_by_experience),
    ToolMessage("Beginner: 4932.69 USD...")
]

# После обрезки (max_history_pairs = 6)
llm_input_messages = [
    # Блок 1: Системное сообщение
    # Всегда сохраняется первым
    SystemMessage("Ты — ассистент..."),

    # Блок 2: Пара 5 (Обычная)
    # Первая сохранившаяся пара
    HumanMessage("А эксперты?"),
    AIMessage("Посмотрю статистику..."),

    # Блок 3: Пара 6 (Инструментальная)
    # Статистика по экспертам
    HumanMessage("Сколько экспертов?"),
    AIMessage(percent_experts_lt_100_projects),
    ToolMessage("33.9% экспертов..."),

    # Блок 4: Пара 7 (Обычная)
    # Вопрос про категории
    HumanMessage("А по категориям?"),
    AIMessage("Сейчас проверю..."),

    # Блок 5: Пара 8 (Инструментальная)
    # Статистика по категориям
    HumanMessage("Покажи по категориям"),
    AIMessage(avg_income_by_category),
    ToolMessage("Design: 5200 USD..."),

    # Блок 6: Пара 9 (Обычная)
    # Вопрос про опыт
    HumanMessage("А по опыту?"),
    AIMessage("Посмотрю..."),

    # Блок 7: Пара 10 (Инструментальная)
    # Последняя пара - статистика по опыту
    HumanMessage("Покажи по опыту"),
    AIMessage(avg_income_by_experience),
    ToolMessage("Beginner: 4932.69 USD...")
]
```

В этом примере:

1. Исходная история содержит 10 пар разных типов
2. После обрезки остаются только последние 6 пар
3. Системное сообщение сохраняется всегда
4. Пары могут быть разного размера (2-3 сообщения)
5. Сохраняется порядок сообщений внутри пар
